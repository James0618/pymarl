import torch
import torch.nn as nn
import torch.nn.functional as F


class TransitionModel(nn.Module):
    def __init__(self, args):
        """
        first, take the hidden state as the agents' state,
        and, the rnn output the prediction of observations.
        """
        super(TransitionModel, self).__init__()
        self.args = args

        # observation here contain "observation" of agents and their actions
        self.from_states = nn.Sequential(
            nn.Linear(args.latent_state_shape, args.rnn_hidden_dim),
            nn.ReLU(),
            nn.Linear(args.rnn_hidden_dim, args.rnn_hidden_dim // 2)
        )
        self.from_agent_actions = nn.Sequential(
            nn.Linear(args.n_actions, args.rnn_hidden_dim),
            nn.ReLU(),
            nn.Linear(args.rnn_hidden_dim, args.rnn_hidden_dim // 4)
        )
        self.from_opponent_actions = nn.Linear(args.latent_action_shape, args.rnn_hidden_dim // 4)

        self.from_actions = nn.Linear(args.rnn_hidden_dim // 2, args.rnn_hidden_dim // 2)

        self.features = nn.Sequential(
            nn.Linear(args.rnn_hidden_dim, args.rnn_hidden_dim),
            nn.ReLU(),
            nn.Linear(args.rnn_hidden_dim, args.rnn_hidden_dim)
        )
        self.transition = nn.Sequential(
            nn.Linear(args.rnn_hidden_dim, args.rnn_hidden_dim),
            nn.ReLU(),
            nn.Linear(args.rnn_hidden_dim, args.latent_state_shape)
        )
        self.pred_reward = nn.Linear(args.rnn_hidden_dim, 1)

    def forward(self, states, agent_actions, opponent_actions):
        from_states = F.relu(self.from_states(states)).reshape(-1, self.args.rnn_hidden_dim // 2)
        from_agent_actions = F.relu(self.from_agent_actions(agent_actions)).reshape(-1, self.args.rnn_hidden_dim // 4)
        from_opponent_actions = F.relu(self.from_opponent_actions(
            opponent_actions)).reshape(-1, self.args.rnn_hidden_dim // 4)
        from_all_actions = F.relu(torch.cat((from_agent_actions, from_opponent_actions), dim=-1))
        from_actions = self.from_actions(from_all_actions)

        observations_and_actions = torch.cat((from_states, from_actions), dim=-1)
        features = F.relu(self.features(observations_and_actions)).reshape(-1, self.args.n_actions,
                                                                           self.args.rnn_hidden_dim)

        next_state = self.transition(features)
        pred_reward = self.pred_reward(features)

        return next_state, pred_reward


class OpponentModel(nn.Module):
    def __init__(self, args):
        """
        take the state generated by transition model as opponents' converted state,
        and transfer the opponents' actions
        """
        super(OpponentModel, self).__init__()
        self.args = args

        self.opponent = nn.Sequential(
            nn.Linear(args.latent_state_shape, args.rnn_hidden_dim * 2),
            nn.ReLU(),
            nn.Linear(args.rnn_hidden_dim * 2, args.latent_action_shape),
        )

    def forward(self, state):
        # TODO: Add GMM to the opponent model!
        latent_actions = self.opponent(state)

        return latent_actions


class StateEstimation(nn.Module):
    def __init__(self, observation_shape, args):
        super(StateEstimation, self).__init__()
        self.args = args

        self.features = nn.Sequential(
            nn.Linear(observation_shape, args.rnn_hidden_dim),
            nn.ReLU(),
            nn.Linear(args.rnn_hidden_dim, args.rnn_hidden_dim),
            nn.ReLU()
        )

        self.gru = nn.GRUCell(args.rnn_hidden_dim, args.rnn_hidden_dim)

        # predict the "observation" of agents and state value
        self.state = nn.Sequential(
            nn.Linear(args.rnn_hidden_dim, args.rnn_hidden_dim),
            nn.ReLU(),
            nn.Linear(args.rnn_hidden_dim, args.latent_state_shape)
        )

    def forward(self, hidden_state, observation):
        # hidden_state = hidden_state.reshape(-1, self.args.rnn_hidden_dim)
        features = self.features(observation).reshape(-1, self.args.rnn_hidden_dim)
        hidden_state = hidden_state.reshape(-1, self.args.rnn_hidden_dim)

        next_hidden_state = self.gru(hidden_state, features)
        next_hidden_state = next_hidden_state.reshape(-1, self.args.n_actions, self.args.rnn_hidden_dim)

        state = self.state(next_hidden_state)

        return state, next_hidden_state

    def init_hidden(self):
        # make hidden states on same device as model
        return torch.zeros(1, self.args.rnn_hidden_dim).cuda()


class ValueEstimation(nn.Module):
    def __init__(self, args):
        super(ValueEstimation, self).__init__()
        self.args = args
        self.estimate_value = nn.Sequential(
            nn.Linear(args.latent_state_shape, args.rnn_hidden_dim),
            nn.ReLU(),
            nn.Linear(args.rnn_hidden_dim, 1)
        )

    def forward(self, state):
        estimate_value = self.estimate_value(state)

        return estimate_value